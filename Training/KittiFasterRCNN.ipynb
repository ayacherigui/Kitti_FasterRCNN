{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning fasterrcnn_resnet50_fpn on the Kitti Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of fine-tuning is to train the last layer (for class prediction and bbox drawing).\n",
    "We can train all of the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split images in `images` directory and XML files in `xmls` directory into `train_images`, `valid_images`, and `train_xmls`, `valid_xmls` directories respectively\n",
    "\n",
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "\n",
    "# # Validation split ratio.\n",
    "# VALID_SPLIT = 0.17\n",
    "\n",
    "# IMAGES_FOLDER = os.path.join('KittiVOC', 'images')\n",
    "# XML_FOLDER = os.path.join('KittiVOC', 'annotations')\n",
    "\n",
    "# TRAIN_IMAGES_DEST = os.path.join('data', 'train_images')\n",
    "# TRAIN_XML_DEST = os.path.join('data', 'train_xmls')\n",
    "# VALID_IMAGES_DEST = os.path.join('data', 'valid_images')\n",
    "# VALID_XMLS_DEST = os.path.join('data', 'valid_xmls')\n",
    "\n",
    "# os.makedirs(TRAIN_IMAGES_DEST, exist_ok=True)\n",
    "# os.makedirs(TRAIN_XML_DEST, exist_ok=True)\n",
    "# os.makedirs(VALID_IMAGES_DEST, exist_ok=True)\n",
    "# os.makedirs(VALID_XMLS_DEST, exist_ok=True)\n",
    "\n",
    "# all_src_images = sorted(os.listdir(IMAGES_FOLDER))\n",
    "# all_src_xmls = sorted(os.listdir(XML_FOLDER))\n",
    "\n",
    "\n",
    "# # Randomoze images and XML list in same order.\n",
    "# temp = list(zip(all_src_images, all_src_xmls))\n",
    "# random.shuffle(temp)\n",
    "# res1, res2 = zip(*temp)\n",
    "# temp_images, temp_xmls = list(res1), list(res2)\n",
    "\n",
    "# print(temp_images[:3])\n",
    "# print(temp_xmls[:3])\n",
    "\n",
    "# num_training_images = int(len(temp_images)*(1-VALID_SPLIT))\n",
    "# num_valid_images = int(len(temp_images)-num_training_images)\n",
    "\n",
    "# print(num_training_images, num_valid_images)\n",
    "\n",
    "# train_images = temp_images[:num_training_images]\n",
    "# train_xmls = temp_xmls[:num_training_images]\n",
    "\n",
    "# valid_images = temp_images[num_training_images:len(all_src_images)]\n",
    "# valid_xmls = temp_xmls[num_training_images:len(all_src_images)]\n",
    "\n",
    "# print(train_images[:3])\n",
    "# print(valid_images[:3])\n",
    "\n",
    "# for i in range(len(train_images)):\n",
    "#     shutil.copy(\n",
    "#         os.path.join(IMAGES_FOLDER, train_images[i]),\n",
    "#         os.path.join(TRAIN_IMAGES_DEST, train_images[i])\n",
    "#     )\n",
    "#     shutil.copy(\n",
    "#         os.path.join(XML_FOLDER, train_xmls[i]),\n",
    "#         os.path.join(TRAIN_XML_DEST, train_xmls[i])\n",
    "#     )\n",
    "\n",
    "# for i in range(len(valid_images)):\n",
    "#     shutil.copy(\n",
    "#         os.path.join(IMAGES_FOLDER, valid_images[i]),\n",
    "#         os.path.join(VALID_IMAGES_DEST, valid_images[i])\n",
    "#     )\n",
    "#     shutil.copy(\n",
    "#         os.path.join(XML_FOLDER, valid_xmls[i]),\n",
    "#         os.path.join(VALID_XMLS_DEST, valid_xmls[i])\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because all the images have different number of boxes we need to make them equal\n",
    "#output = model(images, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_utils.engine import train_one_epoch, evaluate\n",
    "\n",
    "from datasets import (\n",
    "    create_train_dataset, create_valid_dataset, \n",
    "    create_train_loader, create_valid_loader\n",
    ")\n",
    "#from models.create_fasterrcnn_model import create_model\n",
    "from utils.general import (\n",
    "    set_training_dir, Averager, \n",
    "    save_model, save_loss_plot,\n",
    "    show_tranformed_image,\n",
    "    save_mAP, save_model_state, SaveBestModel\n",
    ")\n",
    "from utils.logging import (\n",
    "    set_log, \n",
    "    coco_log\n",
    ")\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import yaml\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# For same annotation colors each time.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open config file\n",
    "with open('/home/aya/Desktop/Kitti_FasterRCNN/data_configs/data.yaml') as file:\n",
    "        data_configs = yaml.safe_load(file)\n",
    "\n",
    "TRAIN_DIR_IMAGES = data_configs['TRAIN_DIR_IMAGES']\n",
    "TRAIN_DIR_LABELS = data_configs['TRAIN_DIR_LABELS']\n",
    "VALID_DIR_IMAGES = data_configs['VALID_DIR_IMAGES']\n",
    "VALID_DIR_LABELS = data_configs['VALID_DIR_LABELS']\n",
    "CLASSES = data_configs['CLASSES']\n",
    "NUM_CLASSES = data_configs['NC']\n",
    "SAVE_VALID_PREDICTIONS = data_configs['SAVE_VALID_PREDICTION_IMAGES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings model's parameters \n",
    "NUM_WORKERS = 4 #num of workers for data processing\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "BATCH_SIZE = 8 #batch size to load the data\n",
    "OUT_DIR = set_training_dir(\"resultat\")\n",
    "COLORS = np.random.uniform(0, 1, size=(len(CLASSES), 3))\n",
    "\n",
    "WEIGHTS_PATH= None\n",
    "RESUME_TRAINING = False\n",
    "COSINE_ANNEALING = True #use cosine annealing warm restarts\n",
    "\n",
    "# Set logging file.\n",
    "set_log(OUT_DIR)\n",
    "# writer = set_summary_writer(OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "IMAGE_WIDTH = 350\n",
    "IMAGE_HEIGHT = 350"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the splitted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_xmls\n",
      "data/valid_xmls\n",
      "Number of training samples: 6209\n",
      "Number of validation samples: 1272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = create_train_dataset(\n",
    "        TRAIN_DIR_IMAGES, TRAIN_DIR_LABELS,\n",
    "        IMAGE_WIDTH, IMAGE_HEIGHT, CLASSES,\n",
    "        use_train_aug=True,\n",
    "        mosaic=False\n",
    "    )\n",
    "\n",
    "valid_dataset = create_valid_dataset(\n",
    "        VALID_DIR_IMAGES, VALID_DIR_LABELS, \n",
    "        IMAGE_WIDTH, IMAGE_HEIGHT, CLASSES\n",
    "    )\n",
    "\n",
    "train_loader = create_train_loader(train_dataset, BATCH_SIZE, NUM_WORKERS)\n",
    "valid_loader = create_valid_loader(valid_dataset, BATCH_SIZE, NUM_WORKERS)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Averager class to keep track of the training and validation loss values and help to get the average for each epoch as well\n",
    "train_loss_hist = Averager()\n",
    "\n",
    "# Train and validation loss lists to store loss values of all iterations till ena and plot graphs for all iterations.\n",
    "train_loss_list = []\n",
    "loss_cls_list = []\n",
    "loss_box_reg_list = []\n",
    "loss_objectness_list = []\n",
    "loss_rpn_list = []\n",
    "train_loss_list_epoch = []\n",
    "val_map_05 = []\n",
    "val_map = []\n",
    "start_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model from scratch...\n"
     ]
    }
   ],
   "source": [
    "if WEIGHTS_PATH is None:\n",
    "        print('Building model from scratch...')        \n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "        # Get the number of input features \n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # define a new head for the detector with required number of classes\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "        \n",
    "# Load pretrained weights if path is provided.\n",
    "if WEIGHTS_PATH is not None:\n",
    "        print('Loading pretrained weights...')\n",
    "        \n",
    "        # Load the pretrained checkpoint.\n",
    "        checkpoint = torch.load(WEIGHTS_PATH, map_location=DEVICE) \n",
    "        keys = list(checkpoint['model_state_dict'].keys())\n",
    "        ckpt_state_dict = checkpoint['model_state_dict']\n",
    "        # Get the number of classes from the loaded checkpoint.\n",
    "        old_classes = ckpt_state_dict['roi_heads.box_predictor.cls_score.weight'].shape[0]\n",
    "\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "        # Build the new model with number of classes same as checkpoint.\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # define a new head for the detector with required number of classes\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "        # Load weights.\n",
    "        model.load_state_dict(ckpt_state_dict)\n",
    "\n",
    "        # Change output features for class predictor and box predictor\n",
    "        # according to current dataset classes.\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor.cls_score = torch.nn.Linear(\n",
    "            in_features=in_features, out_features=NUM_CLASSES, bias=True\n",
    "        )\n",
    "        model.roi_heads.box_predictor.bbox_pred = torch.nn.Linear(\n",
    "            in_features=in_features, out_features=NUM_CLASSES*4, bias=True\n",
    "        )\n",
    "if RESUME_TRAINING:\n",
    "            print('RESUMING TRAINING...')\n",
    "            # Update the starting epochs, the batch-wise loss list, \n",
    "            # and the epoch-wise loss list.\n",
    "            if checkpoint['epoch']:\n",
    "                start_epochs = checkpoint['epoch']\n",
    "                print(f\"Resuming from epoch {start_epochs}...\")\n",
    "            if checkpoint['train_loss_list']:\n",
    "                print('Loading previous batch wise loss list...')\n",
    "                train_loss_list = checkpoint['train_loss_list']\n",
    "            if checkpoint['train_loss_list_epoch']:\n",
    "                print('Loading previous epoch wise loss list...')\n",
    "                train_loss_list_epoch = checkpoint['train_loss_list_epoch']\n",
    "            if checkpoint['val_map']:\n",
    "                print('Loading previous mAP list')\n",
    "                val_map = checkpoint['val_map']\n",
    "            if checkpoint['val_map_05']:\n",
    "                val_map_05 = checkpoint['val_map_05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      "  )\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=9, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=36, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "41,335,036 total parameters.\n",
      "41,112,636 training parameters.\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "# Get the model parameters.\n",
    "params = [p for p in model.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer.\n",
    "#optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, nesterov=True)\n",
    "optimizer = torch.optim.AdamW(params, lr=0.0001, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESUME_TRAINING: \n",
    "        \n",
    "        # LOAD THE OPTIMIZER STATE DICTIONARY FROM THE CHECKPOINT.\n",
    "        print('Loading optimizer state dictionary...')\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        optimizer.param_groups[0]['capturable'] = True\n",
    "\n",
    "\n",
    "if COSINE_ANNEALING:\n",
    "        # LR will be zero as we approach `steps` number of epochs each time.\n",
    "        # If `steps = 5`, LR will slowly reduce to zero every 5 epochs.\n",
    "        steps = NUM_EPOCHS + 10\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, \n",
    "            T_0=steps,\n",
    "            T_mult=1,\n",
    "            verbose=False\n",
    "        )\n",
    "else:\n",
    "        scheduler = None\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 10.91 GiB total capacity; 9.80 GiB already allocated; 23.19 MiB free; 9.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_epochs, NUM_EPOCHS):\n\u001b[1;32m      2\u001b[0m         train_loss_hist\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m         _, batch_loss_list, \\\n\u001b[1;32m      5\u001b[0m              batch_loss_cls_list, \\\n\u001b[1;32m      6\u001b[0m              batch_loss_box_reg_list, \\\n\u001b[1;32m      7\u001b[0m              batch_loss_objectness_list, \\\n\u001b[0;32m----> 8\u001b[0m              batch_loss_rpn_list \u001b[39m=\u001b[39m train_one_epoch(\n\u001b[1;32m      9\u001b[0m             model, \n\u001b[1;32m     10\u001b[0m             optimizer, \n\u001b[1;32m     11\u001b[0m             train_loader, \n\u001b[1;32m     12\u001b[0m             DEVICE, \n\u001b[1;32m     13\u001b[0m             epoch, \n\u001b[1;32m     14\u001b[0m             train_loss_hist,\n\u001b[1;32m     15\u001b[0m             print_freq\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m             scheduler\u001b[39m=\u001b[39;49mscheduler\n\u001b[1;32m     17\u001b[0m         )\n\u001b[1;32m     19\u001b[0m         coco_evaluator, stats, val_pred_image \u001b[39m=\u001b[39m evaluate(\n\u001b[1;32m     20\u001b[0m             model, \n\u001b[1;32m     21\u001b[0m             valid_loader, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m             colors\u001b[39m=\u001b[39mCOLORS\n\u001b[1;32m     27\u001b[0m         )\n\u001b[1;32m     29\u001b[0m         \u001b[39m# Append the current epoch's batch-wise losses to the `train_loss_list`.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Kitti_FasterRCNN/torch_utils/engine.py:50\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, train_loss_hist, print_freq, scaler, scheduler)\u001b[0m\n\u001b[1;32m     48\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[1;32m     49\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(enabled\u001b[39m=\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 50\u001b[0m     loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[1;32m     51\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     53\u001b[0m \u001b[39m# reduce losses over all GPUs for logging purposes\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:101\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     94\u001b[0m             degen_bb: List[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m boxes[bb_idx]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     95\u001b[0m             torch\u001b[39m.\u001b[39m_assert(\n\u001b[1;32m     96\u001b[0m                 \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAll bounding boxes should have positive height and width.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Found invalid box \u001b[39m\u001b[39m{\u001b[39;00mdegen_bb\u001b[39m}\u001b[39;00m\u001b[39m for target at index \u001b[39m\u001b[39m{\u001b[39;00mtarget_idx\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m             )\n\u001b[0;32m--> 101\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(images\u001b[39m.\u001b[39;49mtensors)\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/detection/backbone_utils.py:57\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Tensor]:\n\u001b[0;32m---> 57\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbody(x)\n\u001b[1;32m     58\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfpn(x)\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m out \u001b[39m=\u001b[39m OrderedDict()\n\u001b[1;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m name, module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 69\u001b[0m     x \u001b[39m=\u001b[39m module(x)\n\u001b[1;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers:\n\u001b[1;32m     71\u001b[0m         out_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:155\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    152\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[1;32m    154\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(out)\n\u001b[0;32m--> 155\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn3(out)\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     identity \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/ops/misc.py:62\u001b[0m, in \u001b[0;36mFrozenBatchNorm2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m scale \u001b[39m=\u001b[39m w \u001b[39m*\u001b[39m (rv \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps)\u001b[39m.\u001b[39mrsqrt()\n\u001b[1;32m     61\u001b[0m bias \u001b[39m=\u001b[39m b \u001b[39m-\u001b[39m rm \u001b[39m*\u001b[39m scale\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39;49m scale \u001b[39m+\u001b[39;49m bias\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 10.91 GiB total capacity; 9.80 GiB already allocated; 23.19 MiB free; 9.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epochs, NUM_EPOCHS):\n",
    "        train_loss_hist.reset()\n",
    "\n",
    "        _, batch_loss_list, \\\n",
    "             batch_loss_cls_list, \\\n",
    "             batch_loss_box_reg_list, \\\n",
    "             batch_loss_objectness_list, \\\n",
    "             batch_loss_rpn_list = train_one_epoch(\n",
    "            model, \n",
    "            optimizer, \n",
    "            train_loader, \n",
    "            DEVICE, \n",
    "            epoch, \n",
    "            train_loss_hist,\n",
    "            print_freq=100,\n",
    "            scheduler=scheduler\n",
    "        )\n",
    "\n",
    "        coco_evaluator, stats, val_pred_image = evaluate(\n",
    "            model, \n",
    "            valid_loader, \n",
    "            device=DEVICE,\n",
    "            save_valid_preds=SAVE_VALID_PREDICTIONS,\n",
    "            out_dir=OUT_DIR,\n",
    "            classes=CLASSES,\n",
    "            colors=COLORS\n",
    "        )\n",
    "\n",
    "        # Append the current epoch's batch-wise losses to the `train_loss_list`.\n",
    "        train_loss_list.extend(batch_loss_list)\n",
    "        loss_cls_list.extend(batch_loss_cls_list)\n",
    "        loss_box_reg_list.extend(batch_loss_box_reg_list)\n",
    "        loss_objectness_list.extend(batch_loss_objectness_list)\n",
    "        loss_rpn_list.extend(batch_loss_rpn_list)\n",
    "        # Append curent epoch's average loss to `train_loss_list_epoch`.\n",
    "        train_loss_list_epoch.append(train_loss_hist.value)\n",
    "        val_map_05.append(stats[1])\n",
    "        val_map.append(stats[0])\n",
    "\n",
    "        # Save loss plot for batch-wise list.\n",
    "        save_loss_plot(OUT_DIR, train_loss_list)\n",
    "        # Save loss plot for epoch-wise list.\n",
    "        save_loss_plot(\n",
    "            OUT_DIR, \n",
    "            train_loss_list_epoch,\n",
    "            'epochs',\n",
    "            'train loss',\n",
    "            save_name='train_loss_epoch' \n",
    "        )\n",
    "        save_loss_plot(\n",
    "            OUT_DIR, \n",
    "            loss_cls_list, \n",
    "            'iterations', \n",
    "            'loss cls',\n",
    "            save_name='loss_cls'\n",
    "        )\n",
    "        save_loss_plot(\n",
    "            OUT_DIR, \n",
    "            loss_box_reg_list, \n",
    "            'iterations', \n",
    "            'loss bbox reg',\n",
    "            save_name='loss_bbox_reg'\n",
    "        )\n",
    "        save_loss_plot(\n",
    "            OUT_DIR,\n",
    "            loss_objectness_list,\n",
    "            'iterations',\n",
    "            'loss obj',\n",
    "            save_name='loss_obj'\n",
    "        )\n",
    "        save_loss_plot(\n",
    "            OUT_DIR,\n",
    "            loss_rpn_list,\n",
    "            'iterations',\n",
    "            'loss rpn bbox',\n",
    "            save_name='loss_rpn_bbox'\n",
    "        )\n",
    "\n",
    "        # Save mAP plots.\n",
    "        save_mAP(OUT_DIR, val_map_05, val_map)\n",
    "\n",
    "        coco_log(OUT_DIR, stats)\n",
    "\n",
    "        # Save the current epoch model state. This can be used \n",
    "        # to resume training. It saves model state dict, number of\n",
    "        # epochs trained for, optimizer state dict, and loss function.\n",
    "        save_model(\n",
    "            epoch, \n",
    "            model, \n",
    "            optimizer, \n",
    "            train_loss_list, \n",
    "            train_loss_list_epoch,\n",
    "            val_map,\n",
    "            val_map_05,\n",
    "            OUT_DIR,\n",
    "            data_configs,\n",
    "            \"fasterrcnn_resnet50_fpn\"\n",
    "        )\n",
    "        # Save the model dictionary only for the current epoch.\n",
    "        save_model_state(model, OUT_DIR, data_configs, \"fasterrcnn_resnet50_fpn\")\n",
    "        # Save best model if the current mAP @0.5:0.95 IoU is  greater than the last hightest.\n",
    "        save_best_model(\n",
    "            model, \n",
    "            val_map[-1], \n",
    "            epoch, \n",
    "            OUT_DIR,\n",
    "            data_configs,\n",
    "            \"fasterrcnn_resnet50_fpn\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
