{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning fasterrcnn_resnet50_fpn on the Kitti Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of fine-tuning is to train the last layer (for class prediction and bbox drawing).\n",
    "We can train all of the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: albumentations>=1.1.0 in /home/aya/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: matplotlib in /home/aya/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (3.7.1)\n",
      "Requirement already satisfied: opencv-python>=4.1.1.26 in /home/aya/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.7.0.72)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1.26 in /home/aya/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.7.0.72)\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from -r requirements.txt (line 6)) (9.0.1)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from -r requirements.txt (line 7)) (5.4.1)\n",
      "Requirement already satisfied: scikit-image in /home/aya/.local/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.20.0)\n",
      "Requirement already satisfied: scikit-learn in /home/aya/.local/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (1.1.3)\n",
      "Requirement already satisfied: scipy in /home/aya/.local/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (1.10.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/aya/.local/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.12.0)\n",
      "Requirement already satisfied: torchvision>=0.11.0 in /home/aya/.local/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.13.0)\n",
      "Requirement already satisfied: numpy in /home/aya/.local/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (1.24.2)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in /home/aya/.local/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (3.20.1)\n",
      "Requirement already satisfied: pandas in /home/aya/.local/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (1.5.3)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement sys (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for sys\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split images in `images` directory and XML files in `xmls` directory into `train_images`, `valid_images`, and `train_xmls`, `valid_xmls` directories respectively\n",
    "\n",
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "\n",
    "# # Validation split ratio.\n",
    "# VALID_SPLIT = 0.17\n",
    "\n",
    "# IMAGES_FOLDER = os.path.join('KittiVOC', 'images')\n",
    "# XML_FOLDER = os.path.join('KittiVOC', 'annotations')\n",
    "\n",
    "# TRAIN_IMAGES_DEST = os.path.join('data', 'train_images')\n",
    "# TRAIN_XML_DEST = os.path.join('data', 'train_xmls')\n",
    "# VALID_IMAGES_DEST = os.path.join('data', 'valid_images')\n",
    "# VALID_XMLS_DEST = os.path.join('data', 'valid_xmls')\n",
    "\n",
    "# os.makedirs(TRAIN_IMAGES_DEST, exist_ok=True)\n",
    "# os.makedirs(TRAIN_XML_DEST, exist_ok=True)\n",
    "# os.makedirs(VALID_IMAGES_DEST, exist_ok=True)\n",
    "# os.makedirs(VALID_XMLS_DEST, exist_ok=True)\n",
    "\n",
    "# all_src_images = sorted(os.listdir(IMAGES_FOLDER))\n",
    "# all_src_xmls = sorted(os.listdir(XML_FOLDER))\n",
    "\n",
    "\n",
    "# # Randomoze images and XML list in same order.\n",
    "# temp = list(zip(all_src_images, all_src_xmls))\n",
    "# random.shuffle(temp)\n",
    "# res1, res2 = zip(*temp)\n",
    "# temp_images, temp_xmls = list(res1), list(res2)\n",
    "\n",
    "# print(temp_images[:3])\n",
    "# print(temp_xmls[:3])\n",
    "\n",
    "# num_training_images = int(len(temp_images)*(1-VALID_SPLIT))\n",
    "# num_valid_images = int(len(temp_images)-num_training_images)\n",
    "\n",
    "# print(num_training_images, num_valid_images)\n",
    "\n",
    "# train_images = temp_images[:num_training_images]\n",
    "# train_xmls = temp_xmls[:num_training_images]\n",
    "\n",
    "# valid_images = temp_images[num_training_images:len(all_src_images)]\n",
    "# valid_xmls = temp_xmls[num_training_images:len(all_src_images)]\n",
    "\n",
    "# print(train_images[:3])\n",
    "# print(valid_images[:3])\n",
    "\n",
    "# for i in range(len(train_images)):\n",
    "#     shutil.copy(\n",
    "#         os.path.join(IMAGES_FOLDER, train_images[i]),\n",
    "#         os.path.join(TRAIN_IMAGES_DEST, train_images[i])\n",
    "#     )\n",
    "#     shutil.copy(\n",
    "#         os.path.join(XML_FOLDER, train_xmls[i]),\n",
    "#         os.path.join(TRAIN_XML_DEST, train_xmls[i])\n",
    "#     )\n",
    "\n",
    "# for i in range(len(valid_images)):\n",
    "#     shutil.copy(\n",
    "#         os.path.join(IMAGES_FOLDER, valid_images[i]),\n",
    "#         os.path.join(VALID_IMAGES_DEST, valid_images[i])\n",
    "#     )\n",
    "#     shutil.copy(\n",
    "#         os.path.join(XML_FOLDER, valid_xmls[i]),\n",
    "#         os.path.join(VALID_XMLS_DEST, valid_xmls[i])\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because all the images have different number of boxes we need to make them equal\n",
    "#output = model(images, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_utils.engine import train_one_epoch, evaluate\n",
    "\n",
    "from datasets import (\n",
    "    create_train_dataset, create_valid_dataset, \n",
    "    create_train_loader, create_valid_loader\n",
    ")\n",
    "#from models.create_fasterrcnn_model import create_model\n",
    "from utils.general import (\n",
    "    set_training_dir, Averager, \n",
    "    save_model, save_loss_plot,\n",
    "    show_tranformed_image,\n",
    "    save_mAP, save_model_state, SaveBestModel\n",
    ")\n",
    "from utils.logging import (\n",
    "    set_log, \n",
    "    coco_log\n",
    ")\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import yaml\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# For same annotation colors each time.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open config file\n",
    "with open('/home/aya/Desktop/Kitti-ObjectDetection/FasterRCNN/data_configs/data.yaml') as file:\n",
    "        data_configs = yaml.safe_load(file)\n",
    "\n",
    "TRAIN_DIR_IMAGES = data_configs['TRAIN_DIR_IMAGES']\n",
    "TRAIN_DIR_LABELS = data_configs['TRAIN_DIR_LABELS']\n",
    "VALID_DIR_IMAGES = data_configs['VALID_DIR_IMAGES']\n",
    "VALID_DIR_LABELS = data_configs['VALID_DIR_LABELS']\n",
    "CLASSES = data_configs['CLASSES']\n",
    "NUM_CLASSES = data_configs['NC']\n",
    "SAVE_VALID_PREDICTIONS = data_configs['SAVE_VALID_PREDICTION_IMAGES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings model's parameters \n",
    "NUM_WORKERS = 4 #num of workers for data processing\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "BATCH_SIZE = 8 #batch size to load the data\n",
    "OUT_DIR = set_training_dir(\"resultat\")\n",
    "COLORS = np.random.uniform(0, 1, size=(len(CLASSES), 3))\n",
    "\n",
    "WEIGHTS_PATH=\"/home/aya/Desktop/Kitti_FasterRCNN/outputs/training/resultat2/last_model.pth\"\n",
    "RESUME_TRAINING = False\n",
    "COSINE_ANNEALING = True #use cosine annealing warm restarts\n",
    "\n",
    "# Set logging file.\n",
    "set_log(OUT_DIR)\n",
    "# writer = set_summary_writer(OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "IMAGE_WIDTH = 370\n",
    "IMAGE_HEIGHT = 370"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the splitted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_xmls\n",
      "data/valid_xmls\n",
      "Number of training samples: 6209\n",
      "Number of validation samples: 1272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = create_train_dataset(\n",
    "        TRAIN_DIR_IMAGES, TRAIN_DIR_LABELS,\n",
    "        IMAGE_WIDTH, IMAGE_HEIGHT, CLASSES,\n",
    "        use_train_aug=True,\n",
    "        mosaic=False\n",
    "    )\n",
    "\n",
    "valid_dataset = create_valid_dataset(\n",
    "        VALID_DIR_IMAGES, VALID_DIR_LABELS, \n",
    "        IMAGE_WIDTH, IMAGE_HEIGHT, CLASSES\n",
    "    )\n",
    "\n",
    "train_loader = create_train_loader(train_dataset, BATCH_SIZE, NUM_WORKERS)\n",
    "valid_loader = create_valid_loader(valid_dataset, BATCH_SIZE, NUM_WORKERS)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Averager class to keep track of the training and validation loss values and help to get the average for each epoch as well\n",
    "train_loss_hist = Averager()\n",
    "\n",
    "# Train and validation loss lists to store loss values of all iterations till ena and plot graphs for all iterations.\n",
    "train_loss_list = []\n",
    "loss_cls_list = []\n",
    "loss_box_reg_list = []\n",
    "loss_objectness_list = []\n",
    "loss_rpn_list = []\n",
    "train_loss_list_epoch = []\n",
    "val_map_05 = []\n",
    "val_map = []\n",
    "start_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights...\n"
     ]
    }
   ],
   "source": [
    "if WEIGHTS_PATH is None:\n",
    "        print('Building model from scratch...')        \n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "        # Get the number of input features \n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # define a new head for the detector with required number of classes\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "        \n",
    "# Load pretrained weights if path is provided.\n",
    "if WEIGHTS_PATH is not None:\n",
    "        print('Loading pretrained weights...')\n",
    "        \n",
    "        # Load the pretrained checkpoint.\n",
    "        checkpoint = torch.load(WEIGHTS_PATH, map_location=DEVICE) \n",
    "        keys = list(checkpoint['model_state_dict'].keys())\n",
    "        ckpt_state_dict = checkpoint['model_state_dict']\n",
    "        # Get the number of classes from the loaded checkpoint.\n",
    "        old_classes = ckpt_state_dict['roi_heads.box_predictor.cls_score.weight'].shape[0]\n",
    "\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "        # Build the new model with number of classes same as checkpoint.\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # define a new head for the detector with required number of classes\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "        # Load weights.\n",
    "        model.load_state_dict(ckpt_state_dict)\n",
    "\n",
    "        # Change output features for class predictor and box predictor\n",
    "        # according to current dataset classes.\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor.cls_score = torch.nn.Linear(\n",
    "            in_features=in_features, out_features=NUM_CLASSES, bias=True\n",
    "        )\n",
    "        model.roi_heads.box_predictor.bbox_pred = torch.nn.Linear(\n",
    "            in_features=in_features, out_features=NUM_CLASSES*4, bias=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      "  )\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=15, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=60, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "41,365,786 total parameters.\n",
      "41,143,386 training parameters.\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "# Get the model parameters.\n",
    "params = [p for p in model.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer.\n",
    "#optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, nesterov=True)\n",
    "optimizer = torch.optim.AdamW(params, lr=0.0001, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESUME_TRAINING: \n",
    "        \n",
    "        # LOAD THE OPTIMIZER STATE DICTIONARY FROM THE CHECKPOINT.\n",
    "        print('Loading optimizer state dictionary...')\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "if COSINE_ANNEALING:\n",
    "        # LR will be zero as we approach `steps` number of epochs each time.\n",
    "        # If `steps = 5`, LR will slowly reduce to zero every 5 epochs.\n",
    "        steps = NUM_EPOCHS + 10\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, \n",
    "            T_0=steps,\n",
    "            T_mult=1,\n",
    "            verbose=False\n",
    "        )\n",
    "else:\n",
    "        scheduler = None\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [1]  [  0/777]  eta: 0:16:56  lr: 0.000100  loss: 2.9717 (2.9717)  loss_classifier: 2.3236 (2.3236)  loss_box_reg: 0.6407 (0.6407)  loss_objectness: 0.0023 (0.0023)  loss_rpn_box_reg: 0.0051 (0.0051)  time: 1.3078  data: 0.3865  max mem: 6888\n",
      "Epoch: [1]  [  0/777]  eta: 0:16:56  lr: 0.000100  loss: 2.9717 (2.9717)  loss_classifier: 2.3236 (2.3236)  loss_box_reg: 0.6407 (0.6407)  loss_objectness: 0.0023 (0.0023)  loss_rpn_box_reg: 0.0051 (0.0051)  time: 1.3078  data: 0.3865  max mem: 6888\n",
      "Epoch: [1]  [  0/777]  eta: 0:16:56  lr: 0.000100  loss: 2.9717 (2.9717)  loss_classifier: 2.3236 (2.3236)  loss_box_reg: 0.6407 (0.6407)  loss_objectness: 0.0023 (0.0023)  loss_rpn_box_reg: 0.0051 (0.0051)  time: 1.3078  data: 0.3865  max mem: 6888\n",
      "Epoch: [1]  [  0/777]  eta: 0:16:56  lr: 0.000100  loss: 2.9717 (2.9717)  loss_classifier: 2.3236 (2.3236)  loss_box_reg: 0.6407 (0.6407)  loss_objectness: 0.0023 (0.0023)  loss_rpn_box_reg: 0.0051 (0.0051)  time: 1.3078  data: 0.3865  max mem: 6888\n",
      "Epoch: [1]  [100/777]  eta: 0:10:22  lr: 0.000099  loss: 0.2719 (0.5191)  loss_classifier: 0.0803 (0.2043)  loss_box_reg: 0.1743 (0.2829)  loss_objectness: 0.0017 (0.0123)  loss_rpn_box_reg: 0.0146 (0.0195)  time: 0.9507  data: 0.0061  max mem: 7309\n",
      "Epoch: [1]  [100/777]  eta: 0:10:22  lr: 0.000099  loss: 0.2719 (0.5191)  loss_classifier: 0.0803 (0.2043)  loss_box_reg: 0.1743 (0.2829)  loss_objectness: 0.0017 (0.0123)  loss_rpn_box_reg: 0.0146 (0.0195)  time: 0.9507  data: 0.0061  max mem: 7309\n",
      "Epoch: [1]  [100/777]  eta: 0:10:22  lr: 0.000099  loss: 0.2719 (0.5191)  loss_classifier: 0.0803 (0.2043)  loss_box_reg: 0.1743 (0.2829)  loss_objectness: 0.0017 (0.0123)  loss_rpn_box_reg: 0.0146 (0.0195)  time: 0.9507  data: 0.0061  max mem: 7309\n",
      "Epoch: [1]  [100/777]  eta: 0:10:22  lr: 0.000099  loss: 0.2719 (0.5191)  loss_classifier: 0.0803 (0.2043)  loss_box_reg: 0.1743 (0.2829)  loss_objectness: 0.0017 (0.0123)  loss_rpn_box_reg: 0.0146 (0.0195)  time: 0.9507  data: 0.0061  max mem: 7309\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epochs, NUM_EPOCHS):\n",
    "        train_loss_hist.reset()\n",
    "\n",
    "        _, batch_loss_list, \\\n",
    "             batch_loss_cls_list, \\\n",
    "             batch_loss_box_reg_list, \\\n",
    "             batch_loss_objectness_list, \\\n",
    "             batch_loss_rpn_list = train_one_epoch(\n",
    "            model, \n",
    "            optimizer, \n",
    "            train_loader, \n",
    "            DEVICE, \n",
    "            epoch, \n",
    "            train_loss_hist,\n",
    "            print_freq=100,\n",
    "            scheduler=scheduler\n",
    "        )\n",
    "\n",
    "        coco_evaluator, stats, val_pred_image = evaluate(\n",
    "            model, \n",
    "            valid_loader, \n",
    "            device=DEVICE,\n",
    "            save_valid_preds=SAVE_VALID_PREDICTIONS,\n",
    "            out_dir=OUT_DIR,\n",
    "            classes=CLASSES,\n",
    "            colors=COLORS\n",
    "        )\n",
    "\n",
    "        # Append the current epoch's batch-wise losses to the `train_loss_list`.\n",
    "        train_loss_list.extend(batch_loss_list)\n",
    "        loss_cls_list.extend(batch_loss_cls_list)\n",
    "        loss_box_reg_list.extend(batch_loss_box_reg_list)\n",
    "        loss_objectness_list.extend(batch_loss_objectness_list)\n",
    "        loss_rpn_list.extend(batch_loss_rpn_list)\n",
    "        # Append curent epoch's average loss to `train_loss_list_epoch`.\n",
    "        train_loss_list_epoch.append(train_loss_hist.value)\n",
    "        val_map_05.append(stats[1])\n",
    "        val_map.append(stats[0])\n",
    "\n",
    "        # Save loss plot for batch-wise list.\n",
    "        save_loss_plot(OUT_DIR, train_loss_list)\n",
    "        # Save loss plot for epoch-wise list.\n",
    "        save_loss_plot(\n",
    "            OUT_DIR, \n",
    "            train_loss_list_epoch,\n",
    "            'epochs',\n",
    "            'train loss',\n",
    "            save_name='train_loss_epoch' \n",
    "        )\n",
    "        save_loss_plot(\n",
    "            OUT_DIR, \n",
    "            loss_cls_list, \n",
    "            'iterations', \n",
    "            'loss cls',\n",
    "            save_name='loss_cls'\n",
    "        )\n",
    "        save_loss_plot(\n",
    "            OUT_DIR, \n",
    "            loss_box_reg_list, \n",
    "            'iterations', \n",
    "            'loss bbox reg',\n",
    "            save_name='loss_bbox_reg'\n",
    "        )\n",
    "        save_loss_plot(\n",
    "            OUT_DIR,\n",
    "            loss_objectness_list,\n",
    "            'iterations',\n",
    "            'loss obj',\n",
    "            save_name='loss_obj'\n",
    "        )\n",
    "        save_loss_plot(\n",
    "            OUT_DIR,\n",
    "            loss_rpn_list,\n",
    "            'iterations',\n",
    "            'loss rpn bbox',\n",
    "            save_name='loss_rpn_bbox'\n",
    "        )\n",
    "\n",
    "        # Save mAP plots.\n",
    "        save_mAP(OUT_DIR, val_map_05, val_map)\n",
    "\n",
    "        coco_log(OUT_DIR, stats)\n",
    "\n",
    "        # Save the current epoch model state. This can be used \n",
    "        # to resume training. It saves model state dict, number of\n",
    "        # epochs trained for, optimizer state dict, and loss function.\n",
    "        save_model(\n",
    "            epoch, \n",
    "            model, \n",
    "            optimizer, \n",
    "            train_loss_list, \n",
    "            train_loss_list_epoch,\n",
    "            val_map,\n",
    "            val_map_05,\n",
    "            OUT_DIR,\n",
    "            data_configs,\n",
    "            \"fasterrcnn_resnet50_fpn\"\n",
    "        )\n",
    "        # Save the model dictionary only for the current epoch.\n",
    "        save_model_state(model, OUT_DIR, data_configs, \"fasterrcnn_resnet50_fpn\")\n",
    "        # Save best model if the current mAP @0.5:0.95 IoU is\n",
    "        # greater than the last hightest.\n",
    "        save_best_model(\n",
    "            model, \n",
    "            val_map[-1], \n",
    "            epoch, \n",
    "            OUT_DIR,\n",
    "            data_configs,\n",
    "            \"fasterrcnn_resnet50_fpn\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
